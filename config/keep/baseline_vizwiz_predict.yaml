run: predict
data:
  data_root_dir: /private/home/nvivek/VQA/training_data/
  image_feat_train:
  - rcnn_adaptive_vizwiz/vizwiz_ocr_text,resnet_res5c_vizwiz/vizwiz/resnet152
  image_feat_val:
  - rcnn_adaptive_vizwiz/vizwiz_ocr_text,resnet_res5c_vizwiz/vizwiz/resnet152
  image_feat_test:
  - rcnn_adaptive_vizwiz/vizwiz_ocr_text,resnet_res5c_vizwiz/vizwiz/resnet152
  image_max_loc: 137
  imdb_file_train:
  - imdb/imdb_vizwiz_train_all_answers.npy
  imdb_file_val:
  - imdb/imdb_vizwiz_val_all_answers.npy
  imdb_file_test:
  - imdb/imdb_vizwiz_test_all_answers.npy
  vocab_question_file: vocabulary_100k.txt
  vocab_answer_file: "answers_vizwiz_large.txt"
  image_fast_reader: False
  batch_size: 128
optimizer:
  method: Adamax
  par:
    lr: 0.005
model:
  use_image_text_feat: True
  question_embedding:
  - method: att_que_embed
    par:
      embedding_init_file: 100K_glove.6B.300d.txt.npy
  image_feature_encoding:
  - method: default_image
    par: {}
  - method: default_image
    par: {}
  image_text_feat_encoding:
  - method: image_text_feat_encoding
    par: {}
training_parameters:
    report_interval : 100
    clip_norm_mode: all
    max_grad_l2_norm: 0.25
    lr_steps:
    - 6000
    - 9000
    - 16000
    lr_ratio: 0.01
    wu_factor: 0.2
    wu_iters: 1000
    max_iter: 18000
