data:
  data_root_dir: /private/home/tinayujiang/data/vqa_2.0_vqa_suite
  image_feat_test:
  - rcnn_10_100_bbox/test2015
  image_feat_train:
  - rcnn_10_100_bbox/train2014
  - rcnn_adaptive_mirror/train2014
  - rcnn_10_100_bbox/val2014
  - rcnn_adaptive_mirror/val2014
  - rcnn_10_100_bbox_genome
  - rcnn_10_100_bbox/train2014
  image_feat_val:
  - rcnn_10_100_bbox/val2014
  image_max_loc: 100
  imdb_file_test:
  - imdb/imdb_test2015.npy
  imdb_file_train:
  - imdb/imdb_train2014.npy
  - imdb/imdb_mirror_train2014.npy
  - imdb/imdb_val2014.npy
  - imdb/imdb_mirror_val2014.npy
  - imdb/imdb_genome.npy
  - imdb/imdb_vdtrain.npy
  imdb_file_val:
  - imdb/imdb_minival2014.npy
  vocab_question_file: large_vocabulary_vqa.txt
  image_fast_reader: False
model:
  question_embedding:
  - method: att_que_embed
    par:
      embedding_init_file: large_vqa2.0_glove.6B.300d.txt.npy
training_parameters:
    report_interval : 100
    clip_norm_mode: all
    max_grad_l2_norm: 0.25
    lr_steps:
    - 15000
    - 18000
    - 20000
    - 21000
    lr_ratio: 0.1
    wu_factor: 0.2
    wu_iters: 1000
    max_iter: 22000
